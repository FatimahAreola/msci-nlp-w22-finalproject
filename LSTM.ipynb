{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJE10JlJ_D5R"
      },
      "outputs": [],
      "source": [
        "#Import Reqs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, BatchNormalization, Activation, Bidirectional\n",
        "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, TimeDistributed, Activation, Dot, Reshape, Flatten \n",
        "from keras.utils.vis_utils import plot_model\n",
        "from IPython.display import Image\n",
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LPQIU_K95zxW"
      },
      "outputs": [],
      "source": [
        "#Initialize Hyperparameters\n",
        "MAX_VOCAB_SIZE = 30000\n",
        "MAX_SEN_LEN = 30\n",
        "\n",
        "LSTM_DIM = 128\n",
        "EMBEDDING_DIM = 100\n",
        "BATCH_SIZE = 32\n",
        "N_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "erSAHyzJEo7O"
      },
      "outputs": [],
      "source": [
        "#Load Data Functions - taken from baseline model\n",
        "from csv import DictReader\n",
        "\n",
        "class DataSet():\n",
        "    def __init__(self, name=\"train\", path=\"fnc-1\"):\n",
        "        #self.path = path\n",
        "\n",
        "        print(\"Reading dataset\")\n",
        "        bodies = name+\"_bodies.csv\"\n",
        "        stances = name+\"_stances.csv\"\n",
        "\n",
        "        self.stances = self.read(stances)\n",
        "        articles = self.read(bodies)\n",
        "        self.articles = dict()\n",
        "\n",
        "        #make the body ID an integer value\n",
        "        for s in self.stances:\n",
        "            s['Body ID'] = int(s['Body ID'])\n",
        "\n",
        "        #copy all bodies into a dictionary\n",
        "        for article in articles:\n",
        "            self.articles[int(article['Body ID'])] = article['articleBody']\n",
        "\n",
        "        print(\"Total stances: \" + str(len(self.stances)))\n",
        "        print(\"Total bodies: \" + str(len(self.articles)))\n",
        "\n",
        "\n",
        "\n",
        "    def read(self,filename):\n",
        "        rows = []\n",
        "        with open(filename, \"r\", encoding='utf-8') as table:\n",
        "            r = DictReader(table)\n",
        "\n",
        "            for line in r:\n",
        "                rows.append(line)\n",
        "        return rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7B9_917PIPun"
      },
      "outputs": [],
      "source": [
        "#Pre-processing functions taken from baseline\n",
        "_wnl = nltk.WordNetLemmatizer()\n",
        "def normalize_word(w):\n",
        "    return _wnl.lemmatize(w).lower()\n",
        "\n",
        "def get_tokenized(s):\n",
        "    return [normalize_word(t) for t in nltk.word_tokenize(s)]\n",
        "\n",
        "def clean(s):\n",
        "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n",
        "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()\n",
        "\n",
        "#def remove_stopwords(line):\n",
        "#    # Removes stopwords from a list of tokens\n",
        "#    return [word for word in line if word not in feature_extraction.text.ENGLISH_STOP_WORDS]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DlYEB4o54BQ"
      },
      "outputs": [],
      "source": [
        "#Load & Preprocess Data\n",
        "trainingDataset = DataSet()\n",
        "\n",
        "trainHeadlines_unaltered = []\n",
        "trainBodyID = []\n",
        "labels = []\n",
        "trainingText = []\n",
        "\n",
        "\n",
        "\n",
        "for raw in trainingDataset.stances:\n",
        "  Headline = raw['Headline']\n",
        "  BodyID = raw['Body ID']\n",
        "  Stance = raw['Stance']\n",
        "  Article = trainingDataset.articles[BodyID]\n",
        "  allText = Headline+ \" ENDSEQUENCE \"+ Article\n",
        "  \n",
        "  trainHeadlines_unaltered.append(Headline)\n",
        "  trainBodyID.append(BodyID)\n",
        "  labels.append(Stance)\n",
        "\n",
        "  allText = clean(allText)\n",
        "  allText = get_tokenized(allText)\n",
        "  trainingText.append(allText)\n",
        "\n",
        "\n",
        "competitionDataset = DataSet(\"competition_test\")\n",
        "testHeadlines_unaltered = []\n",
        "testBodyID_unaltered = []\n",
        "testBodyID = []\n",
        "testText = []\n",
        "\n",
        "for raw in competitionDataset.stances:\n",
        "  Headline = raw['Headline']\n",
        "  BodyID = raw['Body ID']\n",
        "  Article = competitionDataset.articles[BodyID]\n",
        "\n",
        "  compiledText = Headline + \" ENDSEQUENCE \"+ Article\n",
        "  \n",
        "  testHeadlines_unaltered.append(Headline)\n",
        "  testBodyID.append(BodyID)\n",
        "\n",
        "  compiledText = clean(compiledText)\n",
        "  FinalT = get_tokenized(compiledText)\n",
        "  testText.append(FinalT)\n",
        "\n",
        "\n",
        "Test = pd.DataFrame(\n",
        "    {'testHeadlines_unaltered': testHeadlines_unaltered,\n",
        "     'BodyID': testBodyID,\n",
        "     'compliledText':testText})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKxX36VAupV6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcyL70r3cMrF"
      },
      "outputs": [],
      "source": [
        "#Tokenizer\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts([' '.join(seq[:MAX_SEN_LEN]) for seq in trainingText])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHgCtO7cuyXZ"
      },
      "outputs": [],
      "source": [
        "print(\"Number of words in vocabulary:\", len(tokenizer.word_index))\n",
        "X = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SEN_LEN]) for seq in trainingText])\n",
        "X = pad_sequences(X, maxlen=MAX_SEN_LEN, padding='post', truncating='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxNzBtrFcZIG"
      },
      "outputs": [],
      "source": [
        "#Split into Train & Validation\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, random_state=10, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJ-I6iRaau5x"
      },
      "outputs": [],
      "source": [
        "#resamples to undersample on \"unrelated\"\n",
        "#resolves bias\n",
        "#under_sampler = RandomUnderSampler(random_state=42)\n",
        "#X_train, y_train = under_sampler.fit_resample(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY4WpRBk_RNv"
      },
      "outputs": [],
      "source": [
        "#Load Embeddings  - word2vec?\n",
        "\n",
        "embeddings = Word2Vec(tokenizer.word_index, min_count=2)\n",
        "vocab_size = len(embeddings.wv.vocab)\n",
        "print('Number of words in this w2v model:', vocab_size)\n",
        "print('Dimension of w2v:', embeddings.vector_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3NyChGLMHNs"
      },
      "outputs": [],
      "source": [
        "#embedding matrix\n",
        "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(tokenizer.word_index)+1, EMBEDDING_DIM)) \n",
        "\n",
        "for word, i in tokenizer.word_index.items(): # i=0 is the embedding for the zero padding\n",
        "    try:\n",
        "        embeddings_vector = embeddings[word]\n",
        "    except KeyError:\n",
        "        embeddings_vector = None\n",
        "    if embeddings_vector is not None:\n",
        "        embeddings_matrix[i] = embeddings_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUNn8yk-cWx6"
      },
      "outputs": [],
      "source": [
        "#One-Hot Encoding\n",
        "y_train = pd.get_dummies(y_train)\n",
        "y_test = pd.get_dummies(y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLdf8xEL59ej"
      },
      "outputs": [],
      "source": [
        "#Define+Complile LSTM Model\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
        "                          output_dim=EMBEDDING_DIM,\n",
        "                          weights = [embeddings_matrix], trainable=False, name='word_embedding_layer', \n",
        "                          mask_zero=True))\n",
        "\n",
        "model.add(LSTM(LSTM_DIM, return_sequences=False, name='lstm_layer'))\n",
        "#model.add(Dense(4, activation='sigmoid', name='sig_layer'))\n",
        "#model.add(Dense(4, activation='ReLU', name='relu_layer'))\n",
        "model.add(Dense(4, activation='tanh',name='tanh_layer'))\n",
        "\n",
        "model.add(Dense(4, activation='softmax', name='output_layer'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIDNea5SbVwC"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDAQHc9B5_pB"
      },
      "outputs": [],
      "source": [
        "#Train Model + test on validation set\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=N_EPOCHS,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "print(\"Total time: \", time.time() - start, \"seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-jEwFsym5ke"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'Validate'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjC81O-1m6Lc"
      },
      "outputs": [],
      "source": [
        "# Loss Plot\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'Validate'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWXMyJL5yRgW"
      },
      "outputs": [],
      "source": [
        "#visualization for the report\n",
        "plot_model(model, to_file='basic_lstm_classifier.png', show_layer_names=True, show_shapes=True)\n",
        "Image('basic_lstm_classifier.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sV5B0Wq9eGtQ"
      },
      "outputs": [],
      "source": [
        "#Prepare Prediction\n",
        "#combinedText =[]\n",
        "#for index,row in Test.iterrows():\n",
        "#  x = row['Headline'] + \" ENDSEQUENCE \" + row['Article']\n",
        "#  combinedText.append(x)\n",
        "\n",
        "\n",
        "X_predict = tokenizer.texts_to_sequences([' '.join(seq[:MAX_SEN_LEN]) for seq in testText])\n",
        "X_predict = pad_sequences(X_predict, maxlen=MAX_SEN_LEN, padding='post', truncating='post')\n",
        "\n",
        "predictions = model.predict(X_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjhWNxdtev8s"
      },
      "outputs": [],
      "source": [
        "stances = []\n",
        "for pred in predictions:\n",
        "  idx = np.argmax(pred)\n",
        "  if idx == 0:\n",
        "    stances.append(\"agree\")\n",
        "  elif idx == 1:\n",
        "    stances.append(\"disagree\")\n",
        "\n",
        "  elif idx == 2:\n",
        "    stances.append(\"discuss\")\n",
        "  else:\n",
        "    stances.append(\"unrelated\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nwo47fTIXvgU"
      },
      "outputs": [],
      "source": [
        "headlines = Test['testHeadlines_unaltered']\n",
        "bodyid= Test['BodyID'],\n",
        "answers = pd.DataFrame({'Headline':Test['testHeadlines_unaltered'], \n",
        "                        'Body ID': Test['BodyID'], \n",
        "                        'Stance': stances})\n",
        "\n",
        "answers.to_csv('answer.csv', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LStg0FJxxAZ8"
      },
      "outputs": [],
      "source": [
        "#Accuracy on test set\n",
        "y_set = pd.read_csv('competition_test_stances.csv')\n",
        "y_pred = pd.get_dummies(y_set['Stance'])\n",
        "\n",
        "score, acc = model.evaluate(X_predict, y_pred,\n",
        "                            batch_size=BATCH_SIZE)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}